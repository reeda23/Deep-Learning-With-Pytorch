{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.Training_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBa2yEhFoKtRkHvvSVzpar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reeda23/Deep-Learning-With-Pytorch/blob/main/5_Training_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Pipeline** <br>\n",
        "\n",
        "1) Design model (input, output size, forward pass) <br>\n",
        "First step is to design our model so we design the number of inputs and outputs\n",
        "and we also design the forward pass with all the different operations or all the different layers. <br>\n",
        "\n",
        "2) Construct loss and Optimizer <br>\n",
        "\n",
        "3) Training loop<br>\n",
        "\n",
        "\n",
        "> -forward pass: compute prediction <br>\n",
        "  -backward pass: gradients <br>\n",
        "  -update weights\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fBQTbtyDH24I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**\n",
        "\n",
        "So we use a function which just does a linear combination of some weights and our inputs and we don't care about bias here. For example\n",
        "\n",
        "f = w * x <br>\n",
        "\n",
        "e.g\n",
        "f = 2 * x"
      ],
      "metadata": {
        "id": "sT3iFICqLcax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "dqCGUpauJwc5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arEcM6ujHteu",
        "outputId": "bff0a3fc-23ff-481b-eab1-bdfb629d2c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions before training: f(5) = 0.000\n",
            "epoch 1: y_pred = tensor([0., 0., 0., 0.], grad_fn=<MulBackward0>), loss =30.00000000, dw= None, w = 0.300\n",
            "epoch 2: y_pred = tensor([0.3000, 0.6000, 0.9000, 1.2000], grad_fn=<MulBackward0>), loss =21.67499924, dw= None, w = 0.855\n",
            "epoch 3: y_pred = tensor([0.8550, 1.7100, 2.5650, 3.4200], grad_fn=<MulBackward0>), loss =9.83268738, dw= None, w = 1.282\n",
            "epoch 4: y_pred = tensor([1.2817, 2.5635, 3.8452, 5.1270], grad_fn=<MulBackward0>), loss =3.86912322, dw= None, w = 1.561\n",
            "epoch 5: y_pred = tensor([1.5612, 3.1225, 4.6837, 6.2449], grad_fn=<MulBackward0>), loss =1.44384420, dw= None, w = 1.735\n",
            "epoch 6: y_pred = tensor([1.7348, 3.4696, 5.2044, 6.9392], grad_fn=<MulBackward0>), loss =0.52752507, dw= None, w = 1.840\n",
            "epoch 7: y_pred = tensor([1.8404, 3.6808, 5.5212, 7.3615], grad_fn=<MulBackward0>), loss =0.19107638, dw= None, w = 1.904\n",
            "epoch 8: y_pred = tensor([1.9041, 3.8082, 5.7123, 7.6164], grad_fn=<MulBackward0>), loss =0.06896293, dw= None, w = 1.942\n",
            "epoch 9: y_pred = tensor([1.9424, 3.8849, 5.8273, 7.7697], grad_fn=<MulBackward0>), loss =0.02485304, dw= None, w = 1.965\n",
            "epoch 10: y_pred = tensor([1.9655, 3.9309, 5.8964, 7.8618], grad_fn=<MulBackward0>), loss =0.00895107, dw= None, w = 1.979\n",
            "epoch 11: y_pred = tensor([1.9793, 3.9585, 5.9378, 7.9171], grad_fn=<MulBackward0>), loss =0.00322300, dw= None, w = 1.988\n",
            "epoch 12: y_pred = tensor([1.9876, 3.9751, 5.9627, 7.9502], grad_fn=<MulBackward0>), loss =0.00116037, dw= None, w = 1.993\n",
            "epoch 13: y_pred = tensor([1.9925, 3.9851, 5.9776, 7.9701], grad_fn=<MulBackward0>), loss =0.00041774, dw= None, w = 1.996\n",
            "epoch 14: y_pred = tensor([1.9955, 3.9910, 5.9866, 7.9821], grad_fn=<MulBackward0>), loss =0.00015039, dw= None, w = 1.997\n",
            "epoch 15: y_pred = tensor([1.9973, 3.9946, 5.9919, 7.9893], grad_fn=<MulBackward0>), loss =0.00005414, dw= None, w = 1.998\n",
            "epoch 16: y_pred = tensor([1.9984, 3.9968, 5.9952, 7.9936], grad_fn=<MulBackward0>), loss =0.00001949, dw= None, w = 1.999\n",
            "epoch 17: y_pred = tensor([1.9990, 3.9981, 5.9971, 7.9961], grad_fn=<MulBackward0>), loss =0.00000702, dw= None, w = 1.999\n",
            "epoch 18: y_pred = tensor([1.9994, 3.9988, 5.9983, 7.9977], grad_fn=<MulBackward0>), loss =0.00000253, dw= None, w = 2.000\n",
            "epoch 19: y_pred = tensor([1.9997, 3.9993, 5.9990, 7.9986], grad_fn=<MulBackward0>), loss =0.00000091, dw= None, w = 2.000\n",
            "epoch 20: y_pred = tensor([1.9998, 3.9996, 5.9994, 7.9992], grad_fn=<MulBackward0>), loss =0.00000033, dw= None, w = 2.000\n",
            "Predictions after training: f(5) = 9.999\n"
          ]
        }
      ],
      "source": [
        "#some training samples\n",
        "\n",
        "x = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
        "\n",
        "#since our formula is 2x so have to multiply x with 2\n",
        "y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
        "\n",
        "#in begining it is zero as we are interested in the gradient of our loss\n",
        "#respect to this parameter so it needs requires_grad = True\n",
        "w = torch.tensor(0.0, dtype = torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "print(f'Predictions before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "#Training\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters =20\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w],lr=learning_rate,)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    #predictions = forward pass\n",
        "    y_pred = forward(x)\n",
        "\n",
        "    #loss calculation\n",
        "    l = loss(y, y_pred)\n",
        "\n",
        "    #local gradient calculations = backward pass\n",
        "    l.backward(retain_graph=True) # it will automatically calculate #dl/dw\n",
        "\n",
        "    #update weights\n",
        "    #now don't need to update weights manually\n",
        "    optimizer.step()\n",
        "\n",
        "    #zero gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print(f'epoch {epoch+1}: y_pred = {y_pred}, loss ={l:.8f}, dw= {l.backward()}, w = {w:.3f}')\n",
        "\n",
        "print(f'Predictions after training: f(5) = {forward(5):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automatic calculation of Gradient Descent with Pytorch Modules**"
      ],
      "metadata": {
        "id": "Xe8PslldMxnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
        "\n",
        "#since our formula is 2x so have to multiply x with 2\n",
        "#y = torch.tensor([2,4,6,8], dtype = torch.float32)\n"
      ],
      "metadata": {
        "id": "Y5lku-0bOXDj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we need to do some modifications as it now nees an input size and an output size of our features\n",
        "#this must be 2d array now where the number of rows is the number of samples\n",
        "#and for each col we have the number of features\n",
        "\n",
        "# tensor([[1.],\n",
        "#         [2.],\n",
        "#         [3.],\n",
        "#         [4.]])\n",
        "\n",
        "# tensor([[2.],\n",
        "#         [4.],\n",
        "#         [6.],\n",
        "#         [8.]])\n",
        "x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
        "y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "\n",
        "x_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = x.shape\n",
        "print(n_samples,n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "#model\n",
        "#this is built in with one layer input and output\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "'''#if we want custom layers\n",
        "class LinearRegression(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        #define layers\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "'''\n",
        "print(f'Predictions before training: f(5) = {model(x_test).item():.3f}')\n",
        "\n",
        "\n",
        "#Training\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters =20\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    #predictions = forward pass\n",
        "    y_pred = model(x)\n",
        "\n",
        "    #loss calculation\n",
        "    l = loss(y, y_pred)\n",
        "\n",
        "    #local gradient calculations = backward pass\n",
        "    l.backward() # it will automatically calculate #dl/dw\n",
        "\n",
        "    #update weights\n",
        "    #now don't need to update weights manually\n",
        "    optimizer.step()\n",
        "\n",
        "    #zero gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        [w, b] = model.parameters()\n",
        "        print(f'epoch {epoch+1}: y_pred = {y_pred}, loss ={l:.8f}, w = {w[0][0].item():.3f}')\n",
        "\n",
        "print(f'Predictions after training: f(5) = {model(x_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CiqE41SL2JA",
        "outputId": "3b0a4f19-6836-40c4-9b4e-1bde9f76d4db"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Predictions before training: f(5) = -0.128\n",
            "epoch 1: y_pred = tensor([[-0.7449],\n",
            "        [-0.5908],\n",
            "        [-0.4366],\n",
            "        [-0.2825]], grad_fn=<AddmmBackward0>), loss =34.66008759, w = 0.476\n",
            "epoch 2: y_pred = tensor([[-0.3128],\n",
            "        [ 0.1632],\n",
            "        [ 0.6391],\n",
            "        [ 1.1151]], grad_fn=<AddmmBackward0>), loss =24.05286026, w = 0.744\n",
            "epoch 3: y_pred = tensor([[0.0472],\n",
            "        [0.7912],\n",
            "        [1.5352],\n",
            "        [2.2792]], grad_fn=<AddmmBackward0>), loss =16.69271469, w = 0.967\n",
            "epoch 4: y_pred = tensor([[0.3472],\n",
            "        [1.3145],\n",
            "        [2.2817],\n",
            "        [3.2489]], grad_fn=<AddmmBackward0>), loss =11.58564568, w = 1.153\n",
            "epoch 5: y_pred = tensor([[0.5972],\n",
            "        [1.7503],\n",
            "        [2.9035],\n",
            "        [4.0566]], grad_fn=<AddmmBackward0>), loss =8.04194355, w = 1.308\n",
            "epoch 6: y_pred = tensor([[0.8055],\n",
            "        [2.1134],\n",
            "        [3.4214],\n",
            "        [4.7294]], grad_fn=<AddmmBackward0>), loss =5.58302498, w = 1.437\n",
            "epoch 7: y_pred = tensor([[0.9790],\n",
            "        [2.4159],\n",
            "        [3.8528],\n",
            "        [5.2898]], grad_fn=<AddmmBackward0>), loss =3.87681675, w = 1.544\n",
            "epoch 8: y_pred = tensor([[1.1237],\n",
            "        [2.6680],\n",
            "        [4.2122],\n",
            "        [5.7565]], grad_fn=<AddmmBackward0>), loss =2.69289732, w = 1.634\n",
            "epoch 9: y_pred = tensor([[1.2443],\n",
            "        [2.8779],\n",
            "        [4.5116],\n",
            "        [6.1452]], grad_fn=<AddmmBackward0>), loss =1.87138486, w = 1.708\n",
            "epoch 10: y_pred = tensor([[1.3448],\n",
            "        [3.0529],\n",
            "        [4.7610],\n",
            "        [6.4690]], grad_fn=<AddmmBackward0>), loss =1.30133581, w = 1.770\n",
            "epoch 11: y_pred = tensor([[1.4286],\n",
            "        [3.1987],\n",
            "        [4.9687],\n",
            "        [6.7387]], grad_fn=<AddmmBackward0>), loss =0.90577555, w = 1.822\n",
            "epoch 12: y_pred = tensor([[1.4985],\n",
            "        [3.3201],\n",
            "        [5.1417],\n",
            "        [6.9633]], grad_fn=<AddmmBackward0>), loss =0.63128746, w = 1.865\n",
            "epoch 13: y_pred = tensor([[1.5568],\n",
            "        [3.4213],\n",
            "        [5.2858],\n",
            "        [7.1503]], grad_fn=<AddmmBackward0>), loss =0.44080877, w = 1.900\n",
            "epoch 14: y_pred = tensor([[1.6055],\n",
            "        [3.5057],\n",
            "        [5.4059],\n",
            "        [7.3061]], grad_fn=<AddmmBackward0>), loss =0.30862361, w = 1.930\n",
            "epoch 15: y_pred = tensor([[1.6461],\n",
            "        [3.5760],\n",
            "        [5.5059],\n",
            "        [7.4358]], grad_fn=<AddmmBackward0>), loss =0.21688673, w = 1.955\n",
            "epoch 16: y_pred = tensor([[1.6799],\n",
            "        [3.6346],\n",
            "        [5.5892],\n",
            "        [7.5438]], grad_fn=<AddmmBackward0>), loss =0.15321575, w = 1.975\n",
            "epoch 17: y_pred = tensor([[1.7082],\n",
            "        [3.6834],\n",
            "        [5.6586],\n",
            "        [7.6337]], grad_fn=<AddmmBackward0>), loss =0.10901959, w = 1.992\n",
            "epoch 18: y_pred = tensor([[1.7319],\n",
            "        [3.7241],\n",
            "        [5.7164],\n",
            "        [7.7086]], grad_fn=<AddmmBackward0>), loss =0.07833664, w = 2.006\n",
            "epoch 19: y_pred = tensor([[1.7517],\n",
            "        [3.7581],\n",
            "        [5.7645],\n",
            "        [7.7709]], grad_fn=<AddmmBackward0>), loss =0.05703039, w = 2.018\n",
            "epoch 20: y_pred = tensor([[1.7682],\n",
            "        [3.7864],\n",
            "        [5.8046],\n",
            "        [7.8228]], grad_fn=<AddmmBackward0>), loss =0.04223040, w = 2.028\n",
            "Predictions after training: f(5) = 9.894\n"
          ]
        }
      ]
    }
  ]
}